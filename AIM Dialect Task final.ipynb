{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc74944d",
   "metadata": {},
   "source": [
    "## Classification of Arabic Tweets based on its written dialect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48280e19",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22872381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import json\n",
    "import utils\n",
    "import pickle\n",
    "import string\n",
    "import warnings\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
    "\n",
    "from keras.layers import Embedding, Dense, Dropout, Input, LSTM, Bidirectional\n",
    "from keras.layers import MaxPooling1D, Conv1D, Flatten\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89373205",
   "metadata": {},
   "source": [
    "# Reading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c448df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_df = pd.read_csv('dialect_dataset.csv')\n",
    "id_df =  dialect_df.drop('dialect',axis=1)\n",
    "dialect_class_df = dialect_df['dialect']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18cb31",
   "metadata": {},
   "source": [
    "# Retreiving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aeaf407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "url = 'https://recruitment.aimtechnologies.co/ai-tasks'\n",
    "\n",
    "list_of_ids_ints = id_df['id'].tolist()\n",
    "list_of_id_strings = [str(x) for x in list_of_ids_ints]\n",
    "id_list_max_range = []\n",
    "id_list_final_range = []\n",
    "retreived_ids = []\n",
    "retreived_text = []\n",
    "\n",
    "for i in range (len(list_of_id_strings)):\n",
    "    id_list_max_range.append(list_of_id_strings[i])\n",
    "    if i >= 458000:\n",
    "        id_list_final_range.append(list_of_id_strings[i])\n",
    "        print(len(id_list_final_range))\n",
    "        if i == 458196:\n",
    "            r = requests.post(url, data= json.dumps(id_list_final_range))\n",
    "            dialect = r.json()\n",
    "            for key, value in dialect.items():\n",
    "                retreived_ids.append(key)\n",
    "                retreived_text.append(value)\n",
    "    if ((i+1) % 1000 == 0):\n",
    "        r = requests.post(url, data= json.dumps(id_list_max_range))\n",
    "        dialect = r.json()\n",
    "        for key, value in dialect.items():\n",
    "            retreived_ids.append(key)\n",
    "            retreived_text.append(value)\n",
    "        id_list_max_range = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062cf8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         retreived_ids\n",
      "0  1175358310087892992\n",
      "1  1175416117793349632\n",
      "2  1175450108898565888\n",
      "3  1175471073770573824\n",
      "4  1175496913145217024\n",
      "5  1175668034146643968\n",
      "6  1175670153884983296\n",
      "7  1175671762580856832\n",
      "8  1175715664398561280\n",
      "9  1176019816072777728\n",
      "                                      retreived_text\n",
      "0   @Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .\n",
      "1  @7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...\n",
      "2                    @KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ\n",
      "3         @HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’\n",
      "4                 @hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº\n",
      "5  @Badi9595 @KanaanRema ÙŠØ§Ø§Ø®ÙŠ Ø§Ù„Ø¥Ø±Ù‡Ø§Ø¨ÙŠ Ø§Ø°Ø§ ÙƒØ§Ù† Ø¹...\n",
      "6  @SarahNadhum90 @nUBNTdfVgACYQxV Ù…Ø·Ù„Ø¨ÙŠ ÙŠÙ‚Ø¯Ù… Ø§Ø³Øª...\n",
      "7  @KanaanRema @Badi9595 Ø®Ù„Øµ ÙˆØ§Ù„Ù„Ù‡ Ù„Ø¹ÙŠÙˆÙ†ÙƒÙ… Ø§Ù†Ø§ Ù…Ø§...\n",
      "8  @SalahAlarbawi ÙŠÙ…ÙƒÙ† Ø³Ø¤Ø§Ù„ ÙØ§Øª Ø§Ù„ÙƒØ«ÙŠØ± Ø§Ù„Ù„ÙŠ ÙŠØµÙˆØ± ...\n",
      "9  @Eng_alow91 @cb4LwpWrS1hT5lb @EdyCohen Ø§ÙˆÙ„Ø§ Ø§Ù†...\n",
      "                         id dialect        retreived_ids  \\\n",
      "0       1175358310087892992      IQ  1175358310087892992   \n",
      "1       1175416117793349632      IQ  1175416117793349632   \n",
      "2       1175450108898565888      IQ  1175450108898565888   \n",
      "3       1175471073770573824      IQ  1175471073770573824   \n",
      "4       1175496913145217024      IQ  1175496913145217024   \n",
      "...                     ...     ...                  ...   \n",
      "458192  1019484980282580992      BH  1019484980282580992   \n",
      "458193  1021083283709407232      BH  1021083283709407232   \n",
      "458194  1017477537889431552      BH  1017477537889431552   \n",
      "458195  1022430374696239232      BH  1022430374696239232   \n",
      "458196  1022409931029458944      BH  1022409931029458944   \n",
      "\n",
      "                                           retreived_text  \n",
      "0        @Nw8ieJUwaCAAreT Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .  \n",
      "1       @7zNqXP0yrODdRjK ÙŠØ¹Ù†ÙŠ Ù‡Ø°Ø§ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø´Ø± .. Ø­...  \n",
      "2                         @KanaanRema Ù…Ø¨ÙŠÙ† Ù…Ù† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ  \n",
      "3              @HAIDER76128900 ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡ğŸ’  \n",
      "4                      @hmo2406 ÙˆÙŠÙ† Ù‡Ù„ Ø§Ù„ØºÙŠØ¨Ù‡  Ø§Ø® Ù…Ø­Ù…Ø¯ ğŸŒ¸ğŸŒº  \n",
      "...                                                   ...  \n",
      "458192              @Al_mhbaa_7 Ù…Ø¨Ø³ÙˆØ·ÙŠÙ† Ù…Ù†Ùƒ Ø§Ù„Ù„ÙŠ Ø¨Ø§Ø³Ø·Ø§Ù†Ø§ğŸ˜…  \n",
      "458193       @Zzainabali @P_ameerah ÙˆØ§Ù„Ù„Ù‡ Ù…Ø§ÙŠÙ†Ø¯Ù‡ Ø§Ø¨Ø´ ÙŠØ®ØªÙŠ  \n",
      "458194  @Al_mhbaa_7 Ø´Ùˆ Ø¹Ù…Ù„Ù†Ø§ Ù„Ùƒ Ø­Ù†Ø§ ØªÙ‡Ø±Ø¨ÙŠ Ù…Ù†Ù†Ø§ Ø§Ø­Ù†Ø§ Ù…Ø³...  \n",
      "458195        @haneenalmwla Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙ‡Ø§ ÙˆØ¨Ø§Ù„Ø¹Ø§ÙÙŠÙ‡ ğŸ˜‹ğŸ˜‹ğŸ˜‹  \n",
      "458196          @jolnar121 Ø§Ù„Ø³Ø­Ù„Ù‡ Ø¶ÙŠÙÙŠ ÙŠ Ø¨ØªØ·Ù„Ø¹ Ù„Ùƒ Ø³Ø­Ù„ÙŠÙ‡ğŸ˜…ğŸ˜…  \n",
      "\n",
      "[458197 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "retreived_ids_df = pd.DataFrame(retreived_ids, columns =['retreived_ids'])\n",
    "retreived_text_df = pd.DataFrame(retreived_text, columns =['retreived_text'])\n",
    "print(retreived_ids_df.iloc[0:10])\n",
    "print(retreived_text_df.iloc[0:10])\n",
    "retreived_ids_var = retreived_ids_df[\"retreived_ids\"]\n",
    "retreived_text_var = retreived_text_df[\"retreived_text\"]\n",
    "dialect_df = dialect_df.join(retreived_ids_var)\n",
    "dialect_df = dialect_df.join(retreived_text_var)\n",
    "print(dialect_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d107c96",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2152478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@nwiejuwacaaret Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© .. ÙŠÙ†ØªÙØ¶ .. ÙŠØºÙŠØ± .\n",
      "nwiejuwacaaret Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ©  ÙŠÙ†ØªÙØ¶  ÙŠØºÙŠØ± \n",
      "nwiejuwacaaret Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ©  ÙŠÙ†ØªÙØ¶  ÙŠØºÙŠØ±\n",
      " Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ©  ÙŠÙ†ØªÙØ¶  ÙŠØºÙŠØ±\n",
      "Ù„ÙƒÙ† Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠØ© ÙŠÙ†ØªÙØ¶ ÙŠØºÙŠØ±\n"
     ]
    }
   ],
   "source": [
    "lower_string = retreived_text[0].lower()\n",
    "no_number_string = re.sub(r'\\d+','',lower_string)\n",
    "print(no_number_string)\n",
    "no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)\n",
    "print(no_punc_string)\n",
    "no_wspace_string = no_punc_string.strip()\n",
    "print(no_wspace_string)\n",
    "text = re.sub('([@A-Za-z0-9_]+)|[^\\w\\s]|#|http\\S+', '', no_wspace_string) # cleaning up\n",
    "print (text)\n",
    "after = \" \".join(text.split())\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcf7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreived_text_cleaned = []\n",
    "for i in range (len(retreived_text)):\n",
    "    lower_string = retreived_text[i].lower()\n",
    "    no_number_string = re.sub(r'\\d+','',lower_string)\n",
    "    no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)\n",
    "    no_wspace_string = no_punc_string.strip()\n",
    "    text = re.sub('([@A-Za-z0-9_]+)|[^\\w\\s]|#|http\\S+', '', no_wspace_string)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    after = \" \".join(text.split())\n",
    "    retreived_text_cleaned.append(after)\n",
    "retreived_text_cleaned_df = pd.DataFrame(retreived_text_cleaned, columns =['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637a03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to subject the data to preprocessing.\n",
    "# This involves removing both arabic and english punctuation\n",
    "# Normalizing different letter variants with one common letter\n",
    "# first we define a list of arabic and english punctiations that we want to get rid of in our text\n",
    "punctuations = '''`Ã·Ã—Ø›<>_()*&^%][Ù€ØŒ/:\"ØŸ.,'{}~Â¦+|!â€â€¦â€œâ€“Ù€''' + string.punctuation\n",
    "# Arabic stop words with nltk\n",
    "stop_words = stopwords.words()\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             Ù‘    | # Shadda\n",
    "                             Ù    | # Fatha\n",
    "                             Ù‹    | # Tanwin Fath\n",
    "                             Ù    | # Damma\n",
    "                             ÙŒ    | # Tanwin Damm\n",
    "                             Ù    | # Kasra\n",
    "                             Ù    | # Tanwin Kasr\n",
    "                             Ù’    | # Sukun\n",
    "                             Ù€     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    text is an arabic string input\n",
    "    \n",
    "    the preprocessed text is returned\n",
    "    '''   \n",
    "    #remove punctuations\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    text = text.translate(translator)\n",
    "    # remove Tashkeel\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    #remove longation\n",
    "    text = re.sub(\"[Ø¥Ø£Ø¢Ø§]\", \"Ø§\", text)\n",
    "    text = re.sub(\"Ù‰\", \"ÙŠ\", text)\n",
    "    text = re.sub(\"Ø¤\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø¦\", \"Ø¡\", text)\n",
    "    text = re.sub(\"Ø©\", \"Ù‡\", text)\n",
    "    text = re.sub(\"Ú¯\", \"Ùƒ\", text)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "retreived_text_cleaned_df['clean_text'] = retreived_text_cleaned_df['clean_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d0f49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡ ÙŠÙ†ØªÙØ¶ ÙŠØºÙŠØ±</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ÙŠØ¹Ù†ÙŠ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„ÙŠ Ø§Ù„Ø¨Ø´Ø± Ø­ÙŠÙˆÙ†Ù‡ ÙˆØ­Ø´ÙŠÙ‡ ÙˆØªØ·Ù„Ø¨ÙˆÙ† Ø§Ù„ØºØ±Ø¨...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ù…Ø¨ÙŠÙ† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ÙˆÙŠÙ† Ø§Ù„ØºÙŠØ¨Ù‡ Ø§Ø® Ù…Ø­Ù…Ø¯</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ÙŠØ§Ø®ÙŠ Ø§Ù„Ø§Ø±Ù‡Ø§Ø¨ÙŠ Ø§Ø°Ø§ Ø¹Ø±Ø§Ù‚ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ ÙˆÙŠÙ† Ø§Ù„Ù…Ø´...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ù…Ø·Ù„Ø¨ÙŠ ÙŠÙ‚Ø¯Ù… Ø§Ø³ØªÙ‚Ø§Ù„ØªÙ‡ ÙˆÙÙˆÙƒØ§Ù‡Ø§ Ø§Ø¹ØªØ°Ø§Ø±</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ø®Ù„Øµ ÙˆØ§Ù„Ù‡ Ù„Ø¹ÙŠÙˆÙ†ÙƒÙ… Ø§Ù†Ø§ Ù…Ø§Ø¹Ù†Ø¯ÙŠ Ø´ÙŠØ¡ Ù…Ø¹Ù‡ Ø¨Ø§Ù„Ø¹ÙƒØ³ Ù…ØªØ§...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ÙŠÙ…ÙƒÙ† Ø³Ø¡Ø§Ù„ ÙØ§Øª Ø§Ù„ÙƒØ«ÙŠØ± Ø§Ù„ÙŠ ÙŠØµÙˆØ± Ø´Ù†Ùˆ Ù…ÙˆÙ‚ÙÙ‡ ÙˆÙƒØ§Ù†Ù‡ ...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ø§ÙˆÙ„Ø§ Ø§Ù†ÙŠ Ø±Ø¯Øª Ø¹Ù„ÙŠ Ø±Ø¬Ù„ Ø¬Ù†ÙˆØ¨ÙŠ ÙˆØ§Ù„ÙŠ Ø°ÙƒØ± Ø­Ø¬Ø§Ø¨Ù‡Ø§ Ø«Ø§Ù†...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ÙˆØ§Ù„Ù‡ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯Ø§ Ø­Ø³Ø§Ø³ ÙˆÙŠØ­ÙŠØ± Ø§ØªÙÙ‚ Ù…Ø¹Ùƒ Ø§Ù†Øª Ù„Ø§ØªØ±Ùˆ...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ø§Ù† Ø´Ø§Ø¡ Ø§Ù„Ù‡ Ø§Ø®ÙˆÙ‡ ÙŠØ¬Ù…Ø¹Ù†Ù‡ Ø§Ù„Ø¹Ø±Ø§Ù‚ Ø§Ù„ÙŠ Ø¨ÙƒÙ„ Ø¬Ø­ÙŠÙ…Ù‡ Ø­Ù„Ùˆ</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ÙŠØ³Ø¹Ø¯ Ù…Ø³Ø§Ùƒ Ø³ÙŠØ¯ Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø­Ø²ÙŠÙ†</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ø¹Ø¯ÙˆÙ„Ù‡ ÙƒÙ„Ø¨Ù‡ ÙˆØ±Ù… Ø¹Ø¯Ù†Ù‡ ÙƒÙ„Ùƒ Ø®Ù„ Ø§ØªÙˆÙ†Ø³ Ù‡Ù…Ù‡ Ø¨ÙƒÙ„ Ø´ÙŠØ¡ Ù…...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ Ø±ÙˆØ¹Ø§Øª ØªÙˆØ§ØµÙ„Ùƒ</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ÙŠØ³Ø¹Ø¯ Ù…Ø³Ø§Ùƒ Ø¨Ù†Øª Ø§Ù„Ø¹Ù…</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ù…Ø§Ø®Ø° Ø§ÙŠ Ø¨Ø´Ø± ÙˆØ­Ø¯ÙŠ</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ø§ØªØ±ÙƒÙ‡ ÙØªØ±Ù‡ Ø§Ø°Ø§ Ù…Ø§Ø³Ø§Ù„ Ù…Ø§ÙŠØ³ØªØ­Ù‚ Ø§Ù‡ØªÙ…Ø§Ù…Ùƒ</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ÙŠØ§ÙƒÙ„ÙˆÙ† Ø¨Ø®ÙŠØ±Ù†Ù‡ ÙˆÙŠÙ‡ÙŠÙ†ÙˆÙ† Ù…ÙˆØ¸ÙÙŠÙ†Ù‡</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ø¹Ø¨Ø§Ù„Ù‡ ÙŠØ±Ø¬Ø¹ ØµØ¯Ø§Ù… Ø­Ø³ÙŠÙ† Ù„Ø­ÙƒÙ…</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_text dialect\n",
       "0                                 Ø¨Ø§Ù„Ù†Ù‡Ø§ÙŠÙ‡ ÙŠÙ†ØªÙØ¶ ÙŠØºÙŠØ±      IQ\n",
       "1   ÙŠØ¹Ù†ÙŠ Ù…Ø­Ø³ÙˆØ¨ Ø¹Ù„ÙŠ Ø§Ù„Ø¨Ø´Ø± Ø­ÙŠÙˆÙ†Ù‡ ÙˆØ­Ø´ÙŠÙ‡ ÙˆØªØ·Ù„Ø¨ÙˆÙ† Ø§Ù„ØºØ±Ø¨...      IQ\n",
       "2                                    Ù…Ø¨ÙŠÙ† ÙƒÙ„Ø§Ù…Ù‡ Ø®Ù„ÙŠØ¬ÙŠ      IQ\n",
       "3                           ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ ÙˆØ±ÙˆØ­Ùƒ Ø§Ù„Ø­Ù„ÙˆÙ‡      IQ\n",
       "4                                  ÙˆÙŠÙ† Ø§Ù„ØºÙŠØ¨Ù‡ Ø§Ø® Ù…Ø­Ù…Ø¯      IQ\n",
       "5   ÙŠØ§Ø®ÙŠ Ø§Ù„Ø§Ø±Ù‡Ø§Ø¨ÙŠ Ø§Ø°Ø§ Ø¹Ø±Ø§Ù‚ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ ÙˆÙŠÙ† Ø§Ù„Ù…Ø´...      IQ\n",
       "6                  Ù…Ø·Ù„Ø¨ÙŠ ÙŠÙ‚Ø¯Ù… Ø§Ø³ØªÙ‚Ø§Ù„ØªÙ‡ ÙˆÙÙˆÙƒØ§Ù‡Ø§ Ø§Ø¹ØªØ°Ø§Ø±      IQ\n",
       "7   Ø®Ù„Øµ ÙˆØ§Ù„Ù‡ Ù„Ø¹ÙŠÙˆÙ†ÙƒÙ… Ø§Ù†Ø§ Ù…Ø§Ø¹Ù†Ø¯ÙŠ Ø´ÙŠØ¡ Ù…Ø¹Ù‡ Ø¨Ø§Ù„Ø¹ÙƒØ³ Ù…ØªØ§...      IQ\n",
       "8   ÙŠÙ…ÙƒÙ† Ø³Ø¡Ø§Ù„ ÙØ§Øª Ø§Ù„ÙƒØ«ÙŠØ± Ø§Ù„ÙŠ ÙŠØµÙˆØ± Ø´Ù†Ùˆ Ù…ÙˆÙ‚ÙÙ‡ ÙˆÙƒØ§Ù†Ù‡ ...      IQ\n",
       "9   Ø§ÙˆÙ„Ø§ Ø§Ù†ÙŠ Ø±Ø¯Øª Ø¹Ù„ÙŠ Ø±Ø¬Ù„ Ø¬Ù†ÙˆØ¨ÙŠ ÙˆØ§Ù„ÙŠ Ø°ÙƒØ± Ø­Ø¬Ø§Ø¨Ù‡Ø§ Ø«Ø§Ù†...      IQ\n",
       "10  ÙˆØ§Ù„Ù‡ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯Ø§ Ø­Ø³Ø§Ø³ ÙˆÙŠØ­ÙŠØ± Ø§ØªÙÙ‚ Ù…Ø¹Ùƒ Ø§Ù†Øª Ù„Ø§ØªØ±Ùˆ...      IQ\n",
       "11    Ø§Ù† Ø´Ø§Ø¡ Ø§Ù„Ù‡ Ø§Ø®ÙˆÙ‡ ÙŠØ¬Ù…Ø¹Ù†Ù‡ Ø§Ù„Ø¹Ø±Ø§Ù‚ Ø§Ù„ÙŠ Ø¨ÙƒÙ„ Ø¬Ø­ÙŠÙ…Ù‡ Ø­Ù„Ùˆ      IQ\n",
       "12                         ÙŠØ³Ø¹Ø¯ Ù…Ø³Ø§Ùƒ Ø³ÙŠØ¯ Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø­Ø²ÙŠÙ†      IQ\n",
       "13  Ø¹Ø¯ÙˆÙ„Ù‡ ÙƒÙ„Ø¨Ù‡ ÙˆØ±Ù… Ø¹Ø¯Ù†Ù‡ ÙƒÙ„Ùƒ Ø®Ù„ Ø§ØªÙˆÙ†Ø³ Ù‡Ù…Ù‡ Ø¨ÙƒÙ„ Ø´ÙŠØ¡ Ù…...      IQ\n",
       "14                          ÙŠØ³Ù„Ù…Ù„ÙŠ Ù…Ø±ÙˆØ±Ùƒ Ø±ÙˆØ¹Ø§Øª ØªÙˆØ§ØµÙ„Ùƒ      IQ\n",
       "15                                 ÙŠØ³Ø¹Ø¯ Ù…Ø³Ø§Ùƒ Ø¨Ù†Øª Ø§Ù„Ø¹Ù…      IQ\n",
       "16                                   Ù…Ø§Ø®Ø° Ø§ÙŠ Ø¨Ø´Ø± ÙˆØ­Ø¯ÙŠ      IQ\n",
       "17               Ø§ØªØ±ÙƒÙ‡ ÙØªØ±Ù‡ Ø§Ø°Ø§ Ù…Ø§Ø³Ø§Ù„ Ù…Ø§ÙŠØ³ØªØ­Ù‚ Ø§Ù‡ØªÙ…Ø§Ù…Ùƒ      IQ\n",
       "18                      ÙŠØ§ÙƒÙ„ÙˆÙ† Ø¨Ø®ÙŠØ±Ù†Ù‡ ÙˆÙŠÙ‡ÙŠÙ†ÙˆÙ† Ù…ÙˆØ¸ÙÙŠÙ†Ù‡      IQ\n",
       "19                          Ø¹Ø¨Ø§Ù„Ù‡ ÙŠØ±Ø¬Ø¹ ØµØ¯Ø§Ù… Ø­Ø³ÙŠÙ† Ù„Ø­ÙƒÙ…      IQ"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(retreived_text_cleaned_df, columns = ['clean_text'])\n",
    "final_df = final_df.join(dialect_class_df)\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e125606",
   "metadata": {},
   "source": [
    "# Split Dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364c1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.clean_text\n",
    "y = final_df.dialect\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23074348",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_text_list = X_train.tolist()\n",
    "clean_test_text_list = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df8f578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer()\n",
    "# # get tf-df values\n",
    "# result = tfidf.fit_transform(clean_train_text_list)  \n",
    "# # get idf values\n",
    "# print('\\nidf values:')\n",
    "# for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "#     print(ele1, ':', ele2)    \n",
    "# # get indexing\n",
    "# print('\\nWord indexes:')\n",
    "# print(tfidf.vocabulary_)  \n",
    "# # display tf-idf values\n",
    "# print('\\ntf-idf value:')\n",
    "# print(result)  \n",
    "# # in matrix form\n",
    "# print('\\ntf-idf values in matrix form:')\n",
    "# print(result.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804385c0",
   "metadata": {},
   "source": [
    "# TF-IDF with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84f7ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(),\n",
    "                    LogisticRegression())\n",
    "# make param grid\n",
    "param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# create and fit the model\n",
    "model_lr = GridSearchCV(pipe, param_grid, cv=5)\n",
    "model_lr.fit(X_train,y_train)\n",
    "\n",
    "# make prediction and print accuracy\n",
    "prediction = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "820581de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4955836201013787\n",
      "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
      "                ('logisticregression', LogisticRegression(C=10))])\n",
      "{'logisticregression__C': 10}\n"
     ]
    }
   ],
   "source": [
    "print(model_lr.best_score_)\n",
    "print(model_lr.best_estimator_)\n",
    "print(model_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62698830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.39      0.39      7863\n",
      "           1       0.34      0.30      0.32      7956\n",
      "           2       0.59      0.47      0.52      4864\n",
      "           3       0.68      0.79      0.73     17143\n",
      "           4       0.61      0.50      0.55      4691\n",
      "           5       0.41      0.31      0.35      8316\n",
      "           6       0.42      0.58      0.49     12603\n",
      "           7       0.60      0.64      0.62      8258\n",
      "           8       0.54      0.68      0.60     10946\n",
      "           9       0.83      0.50      0.62      3558\n",
      "          10       0.39      0.30      0.34      5775\n",
      "          11       0.43      0.53      0.48     13141\n",
      "          12       0.45      0.45      0.45      9380\n",
      "          13       0.37      0.39      0.38      7961\n",
      "          14       0.68      0.53      0.60      4435\n",
      "          15       0.45      0.28      0.34      4882\n",
      "          16       0.72      0.32      0.44      2830\n",
      "          17       0.43      0.13      0.20      2858\n",
      "\n",
      "    accuracy                           0.50    137460\n",
      "   macro avg       0.52      0.45      0.47    137460\n",
      "weighted avg       0.50      0.50      0.49    137460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy score is {accuracy_score(y_test, prediction):.2f}\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1e4e",
   "metadata": {},
   "source": [
    "# Deploy LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e56895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_final = model_lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c239eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file of Predictive Model is saved at Location: C:\\Users\\mohamed mohssen\\AIM Task\n"
     ]
    }
   ],
   "source": [
    "# Saving the Python objects as serialized files can be done using pickle library\n",
    "# Here let us save the Final ZomatoRatingModel\n",
    "with open('model_lr_final.pkl', 'wb') as fileWriteStream:\n",
    "    pickle.dump(model_lr_final, fileWriteStream)\n",
    "    # Don't forget to close the filestream!\n",
    "    fileWriteStream.close()\n",
    "    \n",
    "print('pickle file of Predictive Model is saved at Location:',os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "773cec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function can be called from any from any front end tool/website\n",
    "def FunctionPredictResult(InputData):\n",
    "    Num_Inputs=InputData\n",
    "    \n",
    "    # Making sure the input data has same columns as it was used for training the model\n",
    "    # Also, if standardization/normalization was done, then same must be done for new input\n",
    "    \n",
    "    # Appending the new data with the Training data\n",
    "#     DataForML=pd.read_pickle('DataForML.pkl')\n",
    "#     InputData=InputData.append(DataForML)\n",
    "    \n",
    "    # Generating dummy variables for rest of the nominal variables\n",
    "    InputData=pd.get_dummies(InputData)\n",
    "            \n",
    "    # Maintaining the same order of columns as it was during the model training\n",
    "    Predictors=['clean_text']\n",
    "    \n",
    "    # Generating the input values to the model\n",
    "    X=InputData[Predictors].values[0:Num_Inputs]\n",
    "    \n",
    "    # Generating the standardized values of X since it was done while model training also\n",
    "#     X=PredictorScalerFit.transform(X)\n",
    "    \n",
    "    # Loading the Function from pickle file\n",
    "    import pickle\n",
    "    with open('model_lr_final.pkl', 'rb') as fileReadStream:\n",
    "        PredictionModel=pickle.load(fileReadStream)\n",
    "        # Don't forget to close the filestream!\n",
    "        fileReadStream.close()\n",
    "            \n",
    "    # Genrating Predictions\n",
    "    Prediction=PredictionModel.predict(X)\n",
    "    PredictionResult=pd.DataFrame(Prediction, columns=['Prediction'])\n",
    "    return(PredictionResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "512cc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calling the function for new sample data\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(\"Ù‡ÙŠ Ø¯ÙŠ Ø§Ù„Ø­ÙŠØ§Ø©\")\n",
    "# NewSampleData=pd.DataFrame(\n",
    "# data=X,\n",
    "# columns=['clean_text'])\n",
    "\n",
    "# print(NewSampleData)\n",
    "\n",
    "# # Calling the Function for prediction\n",
    "# FunctionPredictResult(InputData= NewSampleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f54af507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating the function which can take inputs and return predictions\n",
    "# def FunctionGeneratePrediction(test_tweet):\n",
    "    \n",
    "#     # Creating a data frame for the model input\n",
    "#     SampleInputData=pd.DataFrame(\n",
    "#      data=[[test_tweet]],\n",
    "#      columns=['clean_text'])\n",
    "\n",
    "#     # Calling the function defined above using the input parameters\n",
    "#     Predictions=FunctionPredictResult(InputData= SampleInputData)\n",
    "\n",
    "#     # Returning the prediction\n",
    "#     return(Predictions.to_json())\n",
    "\n",
    "# # Function call\n",
    "# FunctionGeneratePrediction( [12 34 45 56] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0113a",
   "metadata": {},
   "source": [
    "# LSTM Model with Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26b5fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25 26 48 ...  0  0  0]\n",
      " [51 25 12 ...  0  0  0]\n",
      " [49 25 36 ...  0  0  0]\n",
      " ...\n",
      " [30 53 37 ...  0  0  0]\n",
      " [30 44 37 ...  0  0  0]\n",
      " [14 42 55 ... 48 27 36]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 56\n",
    "encoded_train_tweets = [one_hot(d, vocab_size) for d in clean_train_text_list]\n",
    "encoded_test_tweets = [one_hot(d, vocab_size) for d in clean_test_text_list]\n",
    "# pad documents to a max length of n words\n",
    "max_length = 18\n",
    "padded_train_tweets = pad_sequences(encoded_train_tweets, maxlen=max_length, padding='post')\n",
    "padded_test_tweets = pad_sequences(encoded_test_tweets, maxlen=max_length, padding='post')\n",
    "print(padded_train_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a7d8f",
   "metadata": {},
   "source": [
    "# Calculating f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7acdb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a641f3",
   "metadata": {},
   "source": [
    "# Deep Learning Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "083f4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 18, 18)            1008      \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 18, 128)           75264     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 18, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 56)                14392     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 18)                1026      \n",
      "=================================================================\n",
      "Total params: 485,930\n",
      "Trainable params: 485,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_features= 18\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_vector_features,input_length=max_length))\n",
    "model.add(LSTM(128,input_shape=(padded_train_tweets.shape),activation='relu',return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# for units in [128,128,64,32]:\n",
    "# model.add(Dense(units,activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(56,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(18,activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc',f1_m,precision_m, recall_m],optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
    "print(model.summary())\n",
    "# optimizer='adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488b882",
   "metadata": {},
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d2f8276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10024/10024 [==============================] - 418s 41ms/step - loss: 2.7416 - acc: 0.1415 - f1_m: 11.6584 - precision_m: 1684991360.0000 - recall_m: 5.8376 - val_loss: 2.7298 - val_acc: 0.1470 - val_f1_m: 11.7380 - val_precision_m: 1751960320.0000 - val_recall_m: 5.8710\n",
      "Epoch 2/5\n",
      "10024/10024 [==============================] - 413s 41ms/step - loss: 2.7274 - acc: 0.1481 - f1_m: 11.5424 - precision_m: 1623100032.0000 - recall_m: 5.7850 - val_loss: 2.7212 - val_acc: 0.1496 - val_f1_m: 11.5526 - val_precision_m: 1736542976.0000 - val_recall_m: 5.7770\n",
      "Epoch 3/5\n",
      "10024/10024 [==============================] - 418s 42ms/step - loss: 2.7222 - acc: 0.1489 - f1_m: 11.4765 - precision_m: 1594533760.0000 - recall_m: 5.7542 - val_loss: 2.7191 - val_acc: 0.1493 - val_f1_m: 11.4543 - val_precision_m: 1686156032.0000 - val_recall_m: 5.7318\n",
      "Epoch 4/5\n",
      "10024/10024 [==============================] - 421s 42ms/step - loss: 2.7186 - acc: 0.1506 - f1_m: 11.4573 - precision_m: 1566105728.0000 - recall_m: 5.7474 - val_loss: 2.7176 - val_acc: 0.1502 - val_f1_m: 11.6664 - val_precision_m: 1587952384.0000 - val_recall_m: 5.8535\n",
      "Epoch 5/5\n",
      "10024/10024 [==============================] - 414s 41ms/step - loss: 2.7154 - acc: 0.1514 - f1_m: 11.4398 - precision_m: 1553748352.0000 - recall_m: 5.7404 - val_loss: 2.7139 - val_acc: 0.1504 - val_f1_m: 11.3860 - val_precision_m: 1654795648.0000 - val_recall_m: 5.6999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2df1bc89670>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train_tweets,y_train,validation_data=(padded_test_tweets,y_test),epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f689",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a5414ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4296/4296 [==============================] - 63s 15ms/step - loss: 2.7139 - acc: 0.1504 - f1_m: 11.3860 - precision_m: 1654795648.0000 - recall_m: 5.6999\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(padded_test_tweets, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3b569cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.385997772216797\n",
      "0.15038557350635529\n"
     ]
    }
   ],
   "source": [
    "print(f1_score)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf1c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
