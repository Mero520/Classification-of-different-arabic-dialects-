{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc74944d",
   "metadata": {},
   "source": [
    "## Classification of Arabic Tweets based on its written dialect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48280e19",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22872381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import json\n",
    "import utils\n",
    "import pickle\n",
    "import string\n",
    "import warnings\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
    "\n",
    "from keras.layers import Embedding, Dense, Dropout, Input, LSTM, Bidirectional\n",
    "from keras.layers import MaxPooling1D, Conv1D, Flatten\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "  \n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89373205",
   "metadata": {},
   "source": [
    "# Reading csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c448df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialect_df = pd.read_csv('dialect_dataset.csv')\n",
    "id_df =  dialect_df.drop('dialect',axis=1)\n",
    "dialect_class_df = dialect_df['dialect']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc18cb31",
   "metadata": {},
   "source": [
    "# Retreiving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aeaf407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n"
     ]
    }
   ],
   "source": [
    "url = 'https://recruitment.aimtechnologies.co/ai-tasks'\n",
    "\n",
    "list_of_ids_ints = id_df['id'].tolist()\n",
    "list_of_id_strings = [str(x) for x in list_of_ids_ints]\n",
    "id_list_max_range = []\n",
    "id_list_final_range = []\n",
    "retreived_ids = []\n",
    "retreived_text = []\n",
    "\n",
    "for i in range (len(list_of_id_strings)):\n",
    "    id_list_max_range.append(list_of_id_strings[i])\n",
    "    if i >= 458000:\n",
    "        id_list_final_range.append(list_of_id_strings[i])\n",
    "        print(len(id_list_final_range))\n",
    "        if i == 458196:\n",
    "            r = requests.post(url, data= json.dumps(id_list_final_range))\n",
    "            dialect = r.json()\n",
    "            for key, value in dialect.items():\n",
    "                retreived_ids.append(key)\n",
    "                retreived_text.append(value)\n",
    "    if ((i+1) % 1000 == 0):\n",
    "        r = requests.post(url, data= json.dumps(id_list_max_range))\n",
    "        dialect = r.json()\n",
    "        for key, value in dialect.items():\n",
    "            retreived_ids.append(key)\n",
    "            retreived_text.append(value)\n",
    "        id_list_max_range = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062cf8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         retreived_ids\n",
      "0  1175358310087892992\n",
      "1  1175416117793349632\n",
      "2  1175450108898565888\n",
      "3  1175471073770573824\n",
      "4  1175496913145217024\n",
      "5  1175668034146643968\n",
      "6  1175670153884983296\n",
      "7  1175671762580856832\n",
      "8  1175715664398561280\n",
      "9  1176019816072777728\n",
      "                                      retreived_text\n",
      "0   @Nw8ieJUwaCAAreT لكن بالنهاية .. ينتفض .. يغير .\n",
      "1  @7zNqXP0yrODdRjK يعني هذا محسوب على البشر .. ح...\n",
      "2                    @KanaanRema مبين من كلامه خليجي\n",
      "3         @HAIDER76128900 يسلملي مرورك وروحك الحلوه💐\n",
      "4                 @hmo2406 وين هل الغيبه  اخ محمد 🌸🌺\n",
      "5  @Badi9595 @KanaanRema يااخي الإرهابي اذا كان ع...\n",
      "6  @SarahNadhum90 @nUBNTdfVgACYQxV مطلبي يقدم است...\n",
      "7  @KanaanRema @Badi9595 خلص والله لعيونكم انا ما...\n",
      "8  @SalahAlarbawi يمكن سؤال فات الكثير اللي يصور ...\n",
      "9  @Eng_alow91 @cb4LwpWrS1hT5lb @EdyCohen اولا ان...\n",
      "                         id dialect        retreived_ids  \\\n",
      "0       1175358310087892992      IQ  1175358310087892992   \n",
      "1       1175416117793349632      IQ  1175416117793349632   \n",
      "2       1175450108898565888      IQ  1175450108898565888   \n",
      "3       1175471073770573824      IQ  1175471073770573824   \n",
      "4       1175496913145217024      IQ  1175496913145217024   \n",
      "...                     ...     ...                  ...   \n",
      "458192  1019484980282580992      BH  1019484980282580992   \n",
      "458193  1021083283709407232      BH  1021083283709407232   \n",
      "458194  1017477537889431552      BH  1017477537889431552   \n",
      "458195  1022430374696239232      BH  1022430374696239232   \n",
      "458196  1022409931029458944      BH  1022409931029458944   \n",
      "\n",
      "                                           retreived_text  \n",
      "0        @Nw8ieJUwaCAAreT لكن بالنهاية .. ينتفض .. يغير .  \n",
      "1       @7zNqXP0yrODdRjK يعني هذا محسوب على البشر .. ح...  \n",
      "2                         @KanaanRema مبين من كلامه خليجي  \n",
      "3              @HAIDER76128900 يسلملي مرورك وروحك الحلوه💐  \n",
      "4                      @hmo2406 وين هل الغيبه  اخ محمد 🌸🌺  \n",
      "...                                                   ...  \n",
      "458192              @Al_mhbaa_7 مبسوطين منك اللي باسطانا😅  \n",
      "458193       @Zzainabali @P_ameerah والله ماينده ابش يختي  \n",
      "458194  @Al_mhbaa_7 شو عملنا لك حنا تهربي مننا احنا مس...  \n",
      "458195        @haneenalmwla الله يبارك فيها وبالعافيه 😋😋😋  \n",
      "458196          @jolnar121 السحله ضيفي ي بتطلع لك سحليه😅😅  \n",
      "\n",
      "[458197 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "retreived_ids_df = pd.DataFrame(retreived_ids, columns =['retreived_ids'])\n",
    "retreived_text_df = pd.DataFrame(retreived_text, columns =['retreived_text'])\n",
    "print(retreived_ids_df.iloc[0:10])\n",
    "print(retreived_text_df.iloc[0:10])\n",
    "retreived_ids_var = retreived_ids_df[\"retreived_ids\"]\n",
    "retreived_text_var = retreived_text_df[\"retreived_text\"]\n",
    "dialect_df = dialect_df.join(retreived_ids_var)\n",
    "dialect_df = dialect_df.join(retreived_text_var)\n",
    "print(dialect_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d107c96",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2152478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@nwiejuwacaaret لكن بالنهاية .. ينتفض .. يغير .\n",
      "nwiejuwacaaret لكن بالنهاية  ينتفض  يغير \n",
      "nwiejuwacaaret لكن بالنهاية  ينتفض  يغير\n",
      " لكن بالنهاية  ينتفض  يغير\n",
      "لكن بالنهاية ينتفض يغير\n"
     ]
    }
   ],
   "source": [
    "lower_string = retreived_text[0].lower()\n",
    "no_number_string = re.sub(r'\\d+','',lower_string)\n",
    "print(no_number_string)\n",
    "no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)\n",
    "print(no_punc_string)\n",
    "no_wspace_string = no_punc_string.strip()\n",
    "print(no_wspace_string)\n",
    "text = re.sub('([@A-Za-z0-9_]+)|[^\\w\\s]|#|http\\S+', '', no_wspace_string) # cleaning up\n",
    "print (text)\n",
    "after = \" \".join(text.split())\n",
    "print(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bcf7acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreived_text_cleaned = []\n",
    "for i in range (len(retreived_text)):\n",
    "    lower_string = retreived_text[i].lower()\n",
    "    no_number_string = re.sub(r'\\d+','',lower_string)\n",
    "    no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string)\n",
    "    no_wspace_string = no_punc_string.strip()\n",
    "    text = re.sub('([@A-Za-z0-9_]+)|[^\\w\\s]|#|http\\S+', '', no_wspace_string)\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    after = \" \".join(text.split())\n",
    "    retreived_text_cleaned.append(after)\n",
    "retreived_text_cleaned_df = pd.DataFrame(retreived_text_cleaned, columns =['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637a03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to subject the data to preprocessing.\n",
    "# This involves removing both arabic and english punctuation\n",
    "# Normalizing different letter variants with one common letter\n",
    "# first we define a list of arabic and english punctiations that we want to get rid of in our text\n",
    "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "# Arabic stop words with nltk\n",
    "stop_words = stopwords.words()\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Shadda\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    text is an arabic string input\n",
    "    \n",
    "    the preprocessed text is returned\n",
    "    '''   \n",
    "    #remove punctuations\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    text = text.translate(translator)\n",
    "    # remove Tashkeel\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    #remove longation\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "retreived_text_cleaned_df['clean_text'] = retreived_text_cleaned_df['clean_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d0f49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بالنهايه ينتفض يغير</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>يعني محسوب علي البشر حيونه وحشيه وتطلبون الغرب...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مبين كلامه خليجي</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>يسلملي مرورك وروحك الحلوه</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>وين الغيبه اخ محمد</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ياخي الارهابي اذا عراقي سعودي فلسطيني وين المش...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>مطلبي يقدم استقالته وفوكاها اعتذار</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>خلص واله لعيونكم انا ماعندي شيء معه بالعكس متا...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>يمكن سءال فات الكثير الي يصور شنو موقفه وكانه ...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>اولا اني ردت علي رجل جنوبي والي ذكر حجابها ثان...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>واله الموضوع جدا حساس ويحير اتفق معك انت لاترو...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ان شاء اله اخوه يجمعنه العراق الي بكل جحيمه حلو</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>يسعد مساك سيد الحرف الحزين</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>عدوله كلبه ورم عدنه كلك خل اتونس همه بكل شيء م...</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>يسلملي مرورك روعات تواصلك</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>يسعد مساك بنت العم</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ماخذ اي بشر وحدي</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>اتركه فتره اذا ماسال مايستحق اهتمامك</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ياكلون بخيرنه ويهينون موظفينه</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>عباله يرجع صدام حسين لحكم</td>\n",
       "      <td>IQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           clean_text dialect\n",
       "0                                 بالنهايه ينتفض يغير      IQ\n",
       "1   يعني محسوب علي البشر حيونه وحشيه وتطلبون الغرب...      IQ\n",
       "2                                    مبين كلامه خليجي      IQ\n",
       "3                           يسلملي مرورك وروحك الحلوه      IQ\n",
       "4                                  وين الغيبه اخ محمد      IQ\n",
       "5   ياخي الارهابي اذا عراقي سعودي فلسطيني وين المش...      IQ\n",
       "6                  مطلبي يقدم استقالته وفوكاها اعتذار      IQ\n",
       "7   خلص واله لعيونكم انا ماعندي شيء معه بالعكس متا...      IQ\n",
       "8   يمكن سءال فات الكثير الي يصور شنو موقفه وكانه ...      IQ\n",
       "9   اولا اني ردت علي رجل جنوبي والي ذكر حجابها ثان...      IQ\n",
       "10  واله الموضوع جدا حساس ويحير اتفق معك انت لاترو...      IQ\n",
       "11    ان شاء اله اخوه يجمعنه العراق الي بكل جحيمه حلو      IQ\n",
       "12                         يسعد مساك سيد الحرف الحزين      IQ\n",
       "13  عدوله كلبه ورم عدنه كلك خل اتونس همه بكل شيء م...      IQ\n",
       "14                          يسلملي مرورك روعات تواصلك      IQ\n",
       "15                                 يسعد مساك بنت العم      IQ\n",
       "16                                   ماخذ اي بشر وحدي      IQ\n",
       "17               اتركه فتره اذا ماسال مايستحق اهتمامك      IQ\n",
       "18                      ياكلون بخيرنه ويهينون موظفينه      IQ\n",
       "19                          عباله يرجع صدام حسين لحكم      IQ"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(retreived_text_cleaned_df, columns = ['clean_text'])\n",
    "final_df = final_df.join(dialect_class_df)\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e125606",
   "metadata": {},
   "source": [
    "# Split Dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364c1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.clean_text\n",
    "y = final_df.dialect\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23074348",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_text_list = X_train.tolist()\n",
    "clean_test_text_list = X_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df8f578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer()\n",
    "# # get tf-df values\n",
    "# result = tfidf.fit_transform(clean_train_text_list)  \n",
    "# # get idf values\n",
    "# print('\\nidf values:')\n",
    "# for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
    "#     print(ele1, ':', ele2)    \n",
    "# # get indexing\n",
    "# print('\\nWord indexes:')\n",
    "# print(tfidf.vocabulary_)  \n",
    "# # display tf-idf values\n",
    "# print('\\ntf-idf value:')\n",
    "# print(result)  \n",
    "# # in matrix form\n",
    "# print('\\ntf-idf values in matrix form:')\n",
    "# print(result.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804385c0",
   "metadata": {},
   "source": [
    "# TF-IDF with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84f7ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make pipeline\n",
    "pipe = make_pipeline(TfidfVectorizer(),\n",
    "                    LogisticRegression())\n",
    "# make param grid\n",
    "param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# create and fit the model\n",
    "model_lr = GridSearchCV(pipe, param_grid, cv=5)\n",
    "model_lr.fit(X_train,y_train)\n",
    "\n",
    "# make prediction and print accuracy\n",
    "prediction = model_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "820581de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4955836201013787\n",
      "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n",
      "                ('logisticregression', LogisticRegression(C=10))])\n",
      "{'logisticregression__C': 10}\n"
     ]
    }
   ],
   "source": [
    "print(model_lr.best_score_)\n",
    "print(model_lr.best_estimator_)\n",
    "print(model_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62698830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.39      0.39      7863\n",
      "           1       0.34      0.30      0.32      7956\n",
      "           2       0.59      0.47      0.52      4864\n",
      "           3       0.68      0.79      0.73     17143\n",
      "           4       0.61      0.50      0.55      4691\n",
      "           5       0.41      0.31      0.35      8316\n",
      "           6       0.42      0.58      0.49     12603\n",
      "           7       0.60      0.64      0.62      8258\n",
      "           8       0.54      0.68      0.60     10946\n",
      "           9       0.83      0.50      0.62      3558\n",
      "          10       0.39      0.30      0.34      5775\n",
      "          11       0.43      0.53      0.48     13141\n",
      "          12       0.45      0.45      0.45      9380\n",
      "          13       0.37      0.39      0.38      7961\n",
      "          14       0.68      0.53      0.60      4435\n",
      "          15       0.45      0.28      0.34      4882\n",
      "          16       0.72      0.32      0.44      2830\n",
      "          17       0.43      0.13      0.20      2858\n",
      "\n",
      "    accuracy                           0.50    137460\n",
      "   macro avg       0.52      0.45      0.47    137460\n",
      "weighted avg       0.50      0.50      0.49    137460\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy score is {accuracy_score(y_test, prediction):.2f}\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f1e4e",
   "metadata": {},
   "source": [
    "# Deploy LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e56895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_final = model_lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c239eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle file of Predictive Model is saved at Location: C:\\Users\\mohamed mohssen\\AIM Task\n"
     ]
    }
   ],
   "source": [
    "# Saving the Python objects as serialized files can be done using pickle library\n",
    "# Here let us save the Final ZomatoRatingModel\n",
    "with open('model_lr_final.pkl', 'wb') as fileWriteStream:\n",
    "    pickle.dump(model_lr_final, fileWriteStream)\n",
    "    # Don't forget to close the filestream!\n",
    "    fileWriteStream.close()\n",
    "    \n",
    "print('pickle file of Predictive Model is saved at Location:',os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "773cec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function can be called from any from any front end tool/website\n",
    "def FunctionPredictResult(InputData):\n",
    "    Num_Inputs=InputData\n",
    "    \n",
    "    # Making sure the input data has same columns as it was used for training the model\n",
    "    # Also, if standardization/normalization was done, then same must be done for new input\n",
    "    \n",
    "    # Appending the new data with the Training data\n",
    "#     DataForML=pd.read_pickle('DataForML.pkl')\n",
    "#     InputData=InputData.append(DataForML)\n",
    "    \n",
    "    # Generating dummy variables for rest of the nominal variables\n",
    "    InputData=pd.get_dummies(InputData)\n",
    "            \n",
    "    # Maintaining the same order of columns as it was during the model training\n",
    "    Predictors=['clean_text']\n",
    "    \n",
    "    # Generating the input values to the model\n",
    "    X=InputData[Predictors].values[0:Num_Inputs]\n",
    "    \n",
    "    # Generating the standardized values of X since it was done while model training also\n",
    "#     X=PredictorScalerFit.transform(X)\n",
    "    \n",
    "    # Loading the Function from pickle file\n",
    "    import pickle\n",
    "    with open('model_lr_final.pkl', 'rb') as fileReadStream:\n",
    "        PredictionModel=pickle.load(fileReadStream)\n",
    "        # Don't forget to close the filestream!\n",
    "        fileReadStream.close()\n",
    "            \n",
    "    # Genrating Predictions\n",
    "    Prediction=PredictionModel.predict(X)\n",
    "    PredictionResult=pd.DataFrame(Prediction, columns=['Prediction'])\n",
    "    return(PredictionResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "512cc71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calling the function for new sample data\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(\"هي دي الحياة\")\n",
    "# NewSampleData=pd.DataFrame(\n",
    "# data=X,\n",
    "# columns=['clean_text'])\n",
    "\n",
    "# print(NewSampleData)\n",
    "\n",
    "# # Calling the Function for prediction\n",
    "# FunctionPredictResult(InputData= NewSampleData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f54af507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating the function which can take inputs and return predictions\n",
    "# def FunctionGeneratePrediction(test_tweet):\n",
    "    \n",
    "#     # Creating a data frame for the model input\n",
    "#     SampleInputData=pd.DataFrame(\n",
    "#      data=[[test_tweet]],\n",
    "#      columns=['clean_text'])\n",
    "\n",
    "#     # Calling the function defined above using the input parameters\n",
    "#     Predictions=FunctionPredictResult(InputData= SampleInputData)\n",
    "\n",
    "#     # Returning the prediction\n",
    "#     return(Predictions.to_json())\n",
    "\n",
    "# # Function call\n",
    "# FunctionGeneratePrediction( [12 34 45 56] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb0113a",
   "metadata": {},
   "source": [
    "# LSTM Model with Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26b5fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25 26 48 ...  0  0  0]\n",
      " [51 25 12 ...  0  0  0]\n",
      " [49 25 36 ...  0  0  0]\n",
      " ...\n",
      " [30 53 37 ...  0  0  0]\n",
      " [30 44 37 ...  0  0  0]\n",
      " [14 42 55 ... 48 27 36]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 56\n",
    "encoded_train_tweets = [one_hot(d, vocab_size) for d in clean_train_text_list]\n",
    "encoded_test_tweets = [one_hot(d, vocab_size) for d in clean_test_text_list]\n",
    "# pad documents to a max length of n words\n",
    "max_length = 18\n",
    "padded_train_tweets = pad_sequences(encoded_train_tweets, maxlen=max_length, padding='post')\n",
    "padded_test_tweets = pad_sequences(encoded_test_tweets, maxlen=max_length, padding='post')\n",
    "print(padded_train_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a7d8f",
   "metadata": {},
   "source": [
    "# Calculating f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7acdb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a641f3",
   "metadata": {},
   "source": [
    "# Deep Learning Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "083f4ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 18, 18)            1008      \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 18, 128)           75264     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 18, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 56)                14392     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 18)                1026      \n",
      "=================================================================\n",
      "Total params: 485,930\n",
      "Trainable params: 485,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_features= 18\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_vector_features,input_length=max_length))\n",
    "model.add(LSTM(128,input_shape=(padded_train_tweets.shape),activation='relu',return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# for units in [128,128,64,32]:\n",
    "# model.add(Dense(units,activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(56,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(18,activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',metrics=['acc',f1_m,precision_m, recall_m],optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\n",
    "print(model.summary())\n",
    "# optimizer='adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4488b882",
   "metadata": {},
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6d2f8276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10024/10024 [==============================] - 418s 41ms/step - loss: 2.7416 - acc: 0.1415 - f1_m: 11.6584 - precision_m: 1684991360.0000 - recall_m: 5.8376 - val_loss: 2.7298 - val_acc: 0.1470 - val_f1_m: 11.7380 - val_precision_m: 1751960320.0000 - val_recall_m: 5.8710\n",
      "Epoch 2/5\n",
      "10024/10024 [==============================] - 413s 41ms/step - loss: 2.7274 - acc: 0.1481 - f1_m: 11.5424 - precision_m: 1623100032.0000 - recall_m: 5.7850 - val_loss: 2.7212 - val_acc: 0.1496 - val_f1_m: 11.5526 - val_precision_m: 1736542976.0000 - val_recall_m: 5.7770\n",
      "Epoch 3/5\n",
      "10024/10024 [==============================] - 418s 42ms/step - loss: 2.7222 - acc: 0.1489 - f1_m: 11.4765 - precision_m: 1594533760.0000 - recall_m: 5.7542 - val_loss: 2.7191 - val_acc: 0.1493 - val_f1_m: 11.4543 - val_precision_m: 1686156032.0000 - val_recall_m: 5.7318\n",
      "Epoch 4/5\n",
      "10024/10024 [==============================] - 421s 42ms/step - loss: 2.7186 - acc: 0.1506 - f1_m: 11.4573 - precision_m: 1566105728.0000 - recall_m: 5.7474 - val_loss: 2.7176 - val_acc: 0.1502 - val_f1_m: 11.6664 - val_precision_m: 1587952384.0000 - val_recall_m: 5.8535\n",
      "Epoch 5/5\n",
      "10024/10024 [==============================] - 414s 41ms/step - loss: 2.7154 - acc: 0.1514 - f1_m: 11.4398 - precision_m: 1553748352.0000 - recall_m: 5.7404 - val_loss: 2.7139 - val_acc: 0.1504 - val_f1_m: 11.3860 - val_precision_m: 1654795648.0000 - val_recall_m: 5.6999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2df1bc89670>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train_tweets,y_train,validation_data=(padded_test_tweets,y_test),epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186f689",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a5414ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4296/4296 [==============================] - 63s 15ms/step - loss: 2.7139 - acc: 0.1504 - f1_m: 11.3860 - precision_m: 1654795648.0000 - recall_m: 5.6999\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy, f1_score, precision, recall = model.evaluate(padded_test_tweets, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3b569cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.385997772216797\n",
      "0.15038557350635529\n"
     ]
    }
   ],
   "source": [
    "print(f1_score)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccf1c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
